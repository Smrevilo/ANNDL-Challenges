{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9966205,"sourceType":"datasetVersion","datasetId":6130836},{"sourceId":208815105,"sourceType":"kernelVersion"},{"sourceId":176230,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":150053,"modelId":172543}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **TRAINING**","metadata":{}},{"cell_type":"markdown","source":"### Initial configurations","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport warnings\nimport logging\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\nfrom tensorflow.keras import Model as tfkModel\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.utils import register_keras_serializable\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nimport seaborn as sns\nfrom keras.callbacks import Callback\nimport IPython.display as display\nfrom PIL import Image\nimport matplotlib.gridspec as gridspec\nimport json\nimport keras_cv\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:04.655182Z","iopub.execute_input":"2024-11-23T18:39:04.655877Z","iopub.status.idle":"2024-11-23T18:39:45.232819Z","shell.execute_reply.started":"2024-11-23T18:39:04.655840Z","shell.execute_reply":"2024-11-23T18:39:45.231559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tfk.mixed_precision.set_global_policy(\"mixed_bfloat16\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:45.234767Z","iopub.execute_input":"2024-11-23T18:39:45.235454Z","iopub.status.idle":"2024-11-23T18:39:45.239761Z","shell.execute_reply.started":"2024-11-23T18:39:45.235418Z","shell.execute_reply":"2024-11-23T18:39:45.238947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure plot display settings\nsns.set(font_scale=1.4)\nsns.set_style('white')\nplt.rc('font', size=14)\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:45.240899Z","iopub.execute_input":"2024-11-23T18:39:45.241180Z","iopub.status.idle":"2024-11-23T18:39:45.266916Z","shell.execute_reply.started":"2024-11-23T18:39:45.241152Z","shell.execute_reply":"2024-11-23T18:39:45.266016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Set accelerator","metadata":{}},{"cell_type":"code","source":"def auto_select_accelerator():\n    \"\"\"\n    Reference:\n        * https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n        * https://www.kaggle.com/xhlulu/ranzcr-efficientnet-tpu-training\n    \"\"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n\n    return strategy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:45.267873Z","iopub.execute_input":"2024-11-23T18:39:45.268100Z","iopub.status.idle":"2024-11-23T18:39:45.273267Z","shell.execute_reply.started":"2024-11-23T18:39:45.268076Z","shell.execute_reply":"2024-11-23T18:39:45.272539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting che correct strategy for TPU / batch sizes\nstrategy = auto_select_accelerator()\nnumGPU = len(tf.config.list_physical_devices('GPU'))\nnumTPU = len(tf.config.list_logical_devices('TPU'))\nprint(\"Num GPUs Available: \", numGPU)\nprint(\"Num TPUs Available: \", numTPU)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:45.275630Z","iopub.execute_input":"2024-11-23T18:39:45.275896Z","iopub.status.idle":"2024-11-23T18:39:54.700030Z","shell.execute_reply.started":"2024-11-23T18:39:45.275869Z","shell.execute_reply":"2024-11-23T18:39:54.699102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nif numTPU != 0:\n    batch_size = strategy.num_replicas_in_sync * 32\n\nprint(f\"Batch size: {batch_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:54.701151Z","iopub.execute_input":"2024-11-23T18:39:54.701465Z","iopub.status.idle":"2024-11-23T18:39:54.706816Z","shell.execute_reply.started":"2024-11-23T18:39:54.701433Z","shell.execute_reply":"2024-11-23T18:39:54.705818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **DATA PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"train_path = \"/kaggle/input/blood-cells-augmented/8_nocleanval_balanced_heavy_full_and_augMix_training_data.npz\"\nval_path = \"/kaggle/input/blood-cells-augmented/8_noclean_balanced_heavy_full_and_augMix_validation_data.npz\"\nprint(f\"reading {train_path}\")\nprint(f\"reading {val_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:54.707857Z","iopub.execute_input":"2024-11-23T18:39:54.708122Z","iopub.status.idle":"2024-11-23T18:39:54.720787Z","shell.execute_reply.started":"2024-11-23T18:39:54.708095Z","shell.execute_reply":"2024-11-23T18:39:54.719975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train = np.load(train_path, allow_pickle=True)\nX_train = data_train['images']\ny_train = data_train['labels']\n\ndata_val = np.load(val_path, allow_pickle=True)\nX_val = data_val['images']\ny_val = data_val['labels']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:39:54.721860Z","iopub.execute_input":"2024-11-23T18:39:54.722121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot 10 random images from X_train\nplt.figure(figsize=(15, 10))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    random_idx = np.random.randint(0, X_train.shape[0])\n    plt.imshow(X_train[random_idx])\n    plt.title(f\"Label: {np.argmax(y_train[random_idx])}\")\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot 10 random images from X_train\nplt.figure(figsize=(15, 10))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    random_idx = np.random.randint(0, X_val.shape[0])\n    plt.imshow(X_val[random_idx])\n    plt.title(f\"Label: {np.argmax(y_val[random_idx])}\")\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the shapes of the loaded datasets\nprint(\"Training Data Shape:\", X_train.shape)\nprint(\"Training Label Shape:\", y_train.shape)\nprint(\"Validation Data Shape:\", X_val.shape)\nprint(\"Validation Label Shape:\", y_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().shuffle(65536).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).cache().shuffle(4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Custom callbacks","metadata":{}},{"cell_type":"code","source":"# Custom implementation of ReduceLROnPlateau\nclass CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n    def __init__(self, monitor='val_accuracy', factor=0.33, patience=20, min_lr=1e-8, verbose=1):\n        super(CustomReduceLROnPlateau, self).__init__()\n        self.monitor = monitor\n        self.factor = factor\n        self.patience = patience\n        self.min_lr = min_lr\n        self.verbose = verbose\n        self.wait = 0\n        self.best = None\n        self.new_lr = None\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        \n        # Initialize best metric if it's the first epoch\n        if self.best is None:\n            self.best = current\n            return\n\n        # Check if the monitored metric has improved\n        if current > self.best:\n            self.best = current\n            self.wait = 0\n        else:\n            self.wait += 1\n\n            # If patience is exceeded, reduce the learning rate\n            if self.wait >= self.patience:\n                old_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n                if old_lr == self.min_lr:\n                    return\n                self.new_lr = max(old_lr * self.factor, self.min_lr)\n                self.model.optimizer.learning_rate.assign(self.new_lr)\n                \n                if self.verbose > 0:\n                    print(f\"\\nEpoch {epoch + 1}: reducing learning rate to {self.new_lr}.\")\n                \n                self.wait = 0  # Reset patience counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom callback class for real-time plotting\nclass RealTimePlot(Callback):\n    def on_train_begin(self, logs=None):\n        # Initialize the lists that will store the metrics\n        self.epochs = []\n        self.train_loss = []\n        self.val_loss = []\n        self.train_acc = []\n        self.val_acc = []\n\n        # Set up the plot\n        self.fig, (self.ax_loss, self.ax_acc) = plt.subplots(1, 2, figsize=(14, 5))\n        plt.show()\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Append the metrics to the lists\n        self.epochs.append(epoch)\n        self.train_loss.append(logs['loss'])\n        self.val_loss.append(logs['val_loss'])\n        self.train_acc.append(logs['categorical_accuracy'])\n        self.val_acc.append(logs['val_categorical_accuracy'])\n\n        # Clear the previous output\n        display.clear_output(wait=True)\n\n        # Plot training and validation loss\n        self.ax_loss.clear()\n        self.ax_loss.plot(self.epochs, self.train_loss, label='Training Loss')\n        self.ax_loss.plot(self.epochs, self.val_loss, label='Validation Loss')\n        self.ax_loss.set_title('Training and Validation Loss')\n        self.ax_loss.set_xlabel('Epoch')\n        self.ax_loss.set_ylabel('Loss')\n        #self.ax_loss.set_ylim(top=2.5, bottom=0.0)\n        self.ax_loss.legend()\n\n        # Plot training and validation accuracy\n        self.ax_acc.clear()\n        self.ax_acc.plot(self.epochs, self.train_acc, label='Training Accuracy')\n        self.ax_acc.plot(self.epochs, self.val_acc, label='Validation Accuracy')\n        self.ax_acc.set_title('Training and Validation Accuracy')\n        self.ax_acc.set_xlabel('Epoch')\n        self.ax_acc.set_ylabel('Accuracy')\n        self.ax_acc.legend()\n\n        # Redraw the updated plots\n        display.display(self.fig)\n        plt.pause(0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DisplayLearningRateCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        # Get the current learning rate from the optimizer and display it\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        print(f\"Epoch {epoch+1} : Learning rate = {tf.keras.backend.get_value(lr)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model definition","metadata":{}},{"cell_type":"code","source":"# Input shape for the model\ninput_shape = X_train.shape[1:]\n\n# Output shape for the model\noutput_shape = y_train.shape[1]\n\nsteps_per_epoch = y_train.shape[0] // batch_size\n\nprint(\"Input Shape: \", input_shape)\nprint(\"Output Shape: \", output_shape)\nprint(\"Steps per epoch: \", steps_per_epoch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@register_keras_serializable()\nclass CustomCastLayer(tfk.layers.Layer):\n    def call(self, inputs):\n        return tf.cast(inputs * 255, tf.uint8)\n\n@register_keras_serializable()\nclass CustomAugmentLayer(tfk.layers.Layer):\n    def __init__(self, max_rotation=30.0, max_zoom=0.2, **kwargs):\n        super(CustomAugmentLayer, self).__init__(**kwargs)\n        self.max_rotation = max_rotation / 360.0\n        self.max_zoom = max_zoom\n        \n    def call(self, inputs, training=False):\n        if training:\n            inputs = tf.image.random_flip_up_down(tf.image.random_flip_left_right(inputs))\n        return inputs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(\n            shape=input_shape, \n            n_labels=output_shape, \n            base_model_trainable=False, #standard definitions\n            n_dense_layers=1, \n            initial_dense_neurons=1024, \n            min_neurons=64, # architecture definitions\n            include_dropout=True, \n            dropout_rate=0.3, \n            l2_lambda=4e-3, # against overfitting\n            learning_rate=1e-3,\n            mult_next_layer = 1/2,\n            include_batch_normalization = True):\n    \n    # The input layer\n    inputs = tfkl.Input(shape=input_shape, name='Input')   \n    \n    # The two augmentation layers\n    x = CustomCastLayer()(inputs)\n    x = CustomAugmentLayer()(x, training=True)\n\n    # The convnext layer with include top=False to take the convolutional part only\n    base_model = tfk.applications.ConvNeXtXLarge(\n                input_shape=input_shape,\n                weights='imagenet',\n                include_top=False\n            )\n\n    # Here we freeze the convnext to perform Tranfer Learning\n    base_model.trainable = base_model_trainable\n\n    x = base_model(x)\n    x = tfkl.BatchNormalization(name=\"BatchNorm_After_ConvNeXt\")(x) if include_batch_normalization else x # BatchNorm after ConvNeXt\n    x = tfkl.GlobalAveragePooling2D()(x)\n\n    # Hidden layers building\n    neurons = initial_dense_neurons\n    for k in range(n_dense_layers):\n        x = tfkl.Dense(units=neurons, activation=None, name=f'Dense_layer_{k}', \n                       kernel_regularizer=tfk.regularizers.L2(l2_lambda))(x)\n        x = tfkl.BatchNormalization(name=f'BatchNorm_Dense_layer_{k}')(x) if include_batch_normalization else x   # BatchNorm in dense layer\n        x = tfkl.Activation('silu', name=f'Activation_layer_{k}')(x)  # Apply activation after BatchNorm\n        if include_dropout:\n            x = tfkl.Dropout(dropout_rate, name=f'Dropout_layer_{k}')(x)\n        neurons = int(neurons * mult_next_layer)\n\n    outputs = tfkl.Dense(output_shape, activation='softmax', name='output_layer')(x)\n\n    # Final model building\n    model = tfk.Model(inputs=inputs, outputs=outputs, name='TF-CNN')\n\n    # Compile the model\n    loss = tfk.losses.CategoricalFocalCrossentropy(\n                                                alpha=0.25,\n                                                gamma=2.0,\n                                                from_logits=False,\n                                                label_smoothing=0.0,\n                                                axis=-1,\n                                                reduction=\"sum_over_batch_size\",\n                                                name=\"categorical_focal_crossentropy\",\n                                                dtype=None,\n                                            )\n    # Metrics definition\n    METRICS = [tfk.metrics.CategoricalAccuracy()]\n    optimizer = tf.keras.optimizers.AdamW(\n                                learning_rate=learning_rate,\n                                weight_decay=l2_lambda,\n                                beta_1=0.9,\n                                beta_2=0.999,\n                                epsilon=1e-07,\n                                amsgrad=False,\n                                use_ema=False,\n                                ema_momentum=0.99,\n                                name=\"adamw\"\n                            )\n                                \n    model.compile(loss=loss, optimizer=optimizer, metrics=METRICS)\n\n    # Return the model\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **TRANSFER LEARNING**","metadata":{}},{"cell_type":"code","source":"# Best values found so far\nn_dense_layers = 5\ninitial_dense_neurons = 1943\ndropout_rate = 0.4\nl2_lambda = 5e-5\nlearning_rate = 1.59608e-5\nmult_next_layer = 0.44626\ninclude_batch_normalization = True\n\nepochs = 500","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the model with specified input and output shapes\nwith strategy.scope():\n    model = build_model(\n            base_model_trainable=False,\n            n_dense_layers=n_dense_layers,\n            initial_dense_neurons=initial_dense_neurons,\n            include_dropout=True,\n            dropout_rate=dropout_rate,\n            l2_lambda=l2_lambda,\n            learning_rate=learning_rate,\n            mult_next_layer=mult_next_layer,\n            include_batch_normalization = include_batch_normalization\n        )\n\n# Display a summary of the model architecture\nmodel.summary(expand_nested=False, show_trainable=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the patience value for early stopping\npatience = 50\n\n# Create an EarlyStopping callback\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor='val_loss',\n    mode='min',\n    patience=patience,\n    restore_best_weights=True\n)\n\nlr_reducer = CustomReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=30, min_lr=1e-8)\nplot_callback = RealTimePlot()\n\n# Store the callback in a list\ncallbacks = [early_stopping, plot_callback, lr_reducer, DisplayLearningRateCallback()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model with early stopping callback\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    shuffle=True,\n    callbacks=callbacks\n).history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation loss\nplt.figure(figsize=(15, 5))\nplt.plot(history['loss'], label='Training loss', alpha=.8)\nplt.plot(history['val_loss'], label='Validation loss', alpha=.8)\nplt.ylim(top=2.5, bottom=0.0)\nplt.title('Loss')\nplt.legend()\nplt.grid(alpha=.3)\n\n# Plot training and validation accuracy\nplt.figure(figsize=(15, 5))\nplt.plot(history['categorical_accuracy'], label='Training accuracy', alpha=.8)\nplt.plot(history['val_categorical_accuracy'], label='Validation accuracy', alpha=.8)\nplt.title('Accuracy')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model to a file with the accuracy included in the filename\nwith strategy.scope():\n    model_weights_filename = 'ADAM_HEAVY_AUG_MODEL.weights.h5'\n    model.save_weights(model_weights_filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LABELS = [0, 1, 2, 3, 4, 5, 6, 7]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluations(model, ds, y_ds, labels, name):\n    # Predict class probabilities and get predicted classes\n    ds_predictions = model.predict(ds, verbose=0)\n    ds_predictions = np.argmax(ds_predictions, axis=-1)\n    \n    # Extract ground truth classes\n    ds_gt = np.argmax(y_ds, axis=-1)\n    \n    # Calculate and display training set accuracy\n    ds_accuracy = accuracy_score(ds_gt, ds_predictions)\n    print(f'Accuracy score over the {name} set: {round(ds_accuracy, 4)}')\n    \n    # Calculate and display training set precision\n    ds_precision = precision_score(ds_gt, ds_predictions, average='weighted')\n    print(f'Precision score over the {name} set: {round(ds_precision, 4)}')\n    \n    # Calculate and display training set recall\n    ds_recall = recall_score(ds_gt, ds_predictions, average='weighted')\n    print(f'Recall score over the {name} set: {round(ds_recall, 4)}')\n    \n    # Calculate and display training set F1 score\n    ds_f1 = f1_score(ds_gt, ds_predictions, average='weighted')\n    print(f'F1 score over the {name} set: {round(ds_f1, 4)}')\n    \n    # Compute the confusion matrix\n    cm = confusion_matrix(ds_gt, ds_predictions)\n    \n    # Create labels combining confusion matrix values\n    labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n    \n    # Plot the confusion matrix with class labels\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=labels, fmt='', xticklabels=labels, yticklabels=labels, cmap='Blues')\n    plt.xlabel('True labels')\n    plt.ylabel('Predicted labels')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_val, y_ds=y_val, labels=LABELS, name='validation')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the weights of the last two dense layers and the output layer\ndense_layer_1_weights = model.get_layer('Dense_layer_0').get_weights()\ndense_layer_2_weights = model.get_layer('Dense_layer_1').get_weights()\ndense_layer_3_weights = model.get_layer('Dense_layer_2').get_weights()\noutput_layer_weights = model.get_layer('output_layer').get_weights()\n\n# Save the weights to files\nnp.savez('dense_layer_1_weights.npz', *dense_layer_1_weights)\nnp.savez('dense_layer_2_weights.npz', *dense_layer_2_weights)\nnp.savez('dense_layer_3_weights.npz', *dense_layer_3_weights)\nnp.savez('output_layer_weights.npz', *output_layer_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine tunning","metadata":{}},{"cell_type":"code","source":"model.get_layer('convnext_xlarge').trainable = True  # Base model\n\n# Unfreeze only specific layers\ntrainable_layers_count = 0\nfor layer in model.get_layer('convnext_xlarge').layers:\n    if 'stage_4' in layer.name or 'stage_3' in layer.name:  # Adjust based on architecture\n        layer.trainable = True\n        trainable_layers_count+=1\n    else:\n        layer.trainable = False\n\nprint(trainable_layers_count)\n\n# Replace Dropout layers with higher rates\ndef increase_dropout(model, new_rate):\n    for layer in model.layers:\n        if isinstance(layer, tf.keras.layers.Dropout):\n            layer.rate = new_rate\n    return model\n\nmodel = increase_dropout(model, new_rate=0)\n\nwith strategy.scope():\n    # Recompile with weight decay\n    model.compile(\n        optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5),  # Adjust weight decay here\n        loss=tf.keras.losses.CategoricalCrossentropy(),\n        metrics=[tf.keras.metrics.CategoricalAccuracy()]\n    )\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    shuffle=True,\n    callbacks=callbacks\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model to a file with the accuracy included in the filename\nwith strategy.scope():\n    model_weights_filename = 'FINE_ADAM_HEAVY_AUG_MODEL.weights.h5'\n    model.save_weights(model_weights_filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_val, y_ds=y_val, labels=LABELS, name='validation')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}