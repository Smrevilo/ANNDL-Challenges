{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **TRAINING**","metadata":{}},{"cell_type":"code","source":"# Set seed for reproducibility\nseed = 42\n\n# Import necessary libraries\nimport os\n\n# Set environment variables before importing modules\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\nos.environ['GRPC_VERBOSITY'] = 'ERROR'\nos.environ['GRPC_TRACE'] = ''\nos.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = '/etc/ssl/certs/ca-certificates.crt'\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Import necessary modules\nimport logging\nimport random\nimport numpy as np\n\n# Set seeds for random number generators in NumPy and Python\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Import TensorFlow and Keras\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\nfrom tensorflow.keras import Model as tfkModel\n\n# Set seed for TensorFlow\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\n\n# Reduce TensorFlow verbosity\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\n# Print TensorFlow version\nprint(tf.__version__)\nprint(tfk.__version__)\n\n# Import other libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.utils import register_keras_serializable\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nimport seaborn as sns\nfrom keras.callbacks import Callback\nimport IPython.display as display\n# import graphviz\nfrom PIL import Image\nimport matplotlib.gridspec as gridspec\nimport json\n\n# Configure plot display settings\nsns.set(font_scale=1.4)\nsns.set_style('white')\nplt.rc('font', size=14)\n%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **DISTRIBUTION DEFINITIONS**","metadata":{}},{"cell_type":"code","source":"def auto_select_accelerator():\n    \"\"\"\n    Reference:\n        * https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n        * https://www.kaggle.com/xhlulu/ranzcr-efficientnet-tpu-training\n    \"\"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n\n    return strategy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting che correct strategy for TPU / batch sizes\nstrategy = auto_select_accelerator()\nnumGPU = len(tf.config.list_physical_devices('GPU'))\nnumTPU = len(tf.config.list_logical_devices('TPU'))\nprint(\"Num GPUs Available: \", numGPU)\nprint(\"Num TPUs Available: \", numTPU)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nif numTPU != 0:\n    batch_size = strategy.num_replicas_in_sync * 8\n\nprint(f\"Batch size: {batch_size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **DATA PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/data-preprocessing/data_preprocessed.npz\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = np.load(data_path, allow_pickle=True)\nlst = data.files\nX = data[lst[0]]\ny = data[lst[1]]\n\n# Convert values in data to int\nX = X.astype(int)\n\n# Normalize data to the range [0, 1]\nX = (X / 255).astype('float32')\n\n# Create a mapping from label string to values\nmap = {'Basophil':0 , 'Eosinophil':1, 'Erythroblast': 2, 'Immature granulocytes': 3, 'Lymphocyte': 4, 'Monosyte': 5, 'Neutrophil': 6, 'Platelet': 7}\n\n# Convert labels to categorical format using one-hot encoding\n#y = tf.keras.utils.to_categorical(y)\n\n# Encode the labels via LabelEncoding from stratch\ny = np.array(map[label] for label in y)\n\n# Split data into training, validation, and test sets, maintaining class distribution\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=0.1, stratify=y)\n\n# Splitting the training data into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=0.15, stratify=y_train_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del X, y # to free up resources","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the shapes of the loaded datasets\nprint(\"Training Data Shape:\", X_train.shape)\nprint(\"Training Label Shape:\", y_train.shape)\nprint(\"Validation Data Shape:\", X_val.shape)\nprint(\"Validation Label Shape:\", y_val.shape)\nprint(\"Test Data Shape:\", X_test.shape)\nprint(\"Test Label Shape:\", y_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().shuffle(4096).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).cache().shuffle(4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().shuffle(4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Input shape for the model\ninput_shape = X_train.shape[1:]\n\n# Output shape for the model\noutput_shape = y_train.shape[1]\n\nsteps_per_epoch = y_train.shape[0] // batch_size\n\nprint(\"Input Shape: \", input_shape)\nprint(\"Output Shape: \", output_shape)\nprint(\"Steps per epoch: \", steps_per_epoch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **CUSTOM CALLBACKS AND METRICS DEFINITIONS**","metadata":{}},{"cell_type":"code","source":"# Custom implementation of ReduceLROnPlateau\nclass CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n    def __init__(self, monitor='val_binary_accuracy', factor=0.33, patience=20, min_lr=1e-8, verbose=1):\n        super(CustomReduceLROnPlateau, self).__init__()\n        self.monitor = monitor\n        self.factor = factor\n        self.patience = patience\n        self.min_lr = min_lr\n        self.verbose = verbose\n        self.wait = 0\n        self.best = None\n        self.new_lr = None\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        \n        # Initialize best metric if it's the first epoch\n        if self.best is None:\n            self.best = current\n            return\n\n        # Check if the monitored metric has improved\n        if current > self.best:\n            self.best = current\n            self.wait = 0\n        else:\n            self.wait += 1\n\n            # If patience is exceeded, reduce the learning rate\n            if self.wait >= self.patience:\n                old_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n                if old_lr == self.min_lr:\n                    return\n                self.new_lr = max(old_lr * self.factor, self.min_lr)\n                self.model.optimizer.learning_rate.assign(self.new_lr)\n                \n                if self.verbose > 0:\n                    print(f\"\\nEpoch {epoch + 1}: reducing learning rate to {self.new_lr}.\")\n                \n                self.wait = 0  # Reset patience counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DisplayLearningRateCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        # Get the current learning rate from the optimizer and display it\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        print(f\"Epoch {epoch+1} : Learning rate = {tf.keras.backend.get_value(lr)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WeightedMacroF1Score(tf.keras.metrics.Metric):\n    def __init__(self, num_classes, name=\"weighted_macro_f1_score\", **kwargs):\n        super(WeightedMacroF1Score, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n        self.true_positives = self.add_weight(\"tp\", shape=(num_classes,), initializer=\"zeros\")\n        self.false_positives = self.add_weight(\"fp\", shape=(num_classes,), initializer=\"zeros\")\n        self.false_negatives = self.add_weight(\"fn\", shape=(num_classes,), initializer=\"zeros\")\n        self.support = self.add_weight(\"support\", shape=(num_classes,), initializer=\"zeros\")  # To track class weights\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.int32)\n        y_pred = tf.argmax(y_pred, axis=1)\n\n        for i in range(self.num_classes):\n            y_true_i = tf.equal(y_true, i)\n            y_pred_i = tf.equal(y_pred, i)\n\n            # True positives, false positives, and false negatives for class `i`\n            tp = tf.reduce_sum(tf.cast(y_true_i & y_pred_i, tf.float32))\n            fp = tf.reduce_sum(tf.cast(~y_true_i & y_pred_i, tf.float32))\n            fn = tf.reduce_sum(tf.cast(y_true_i & ~y_pred_i, tf.float32))\n            support = tf.reduce_sum(tf.cast(y_true_i, tf.float32))  # Total true samples for class `i`\n\n            self.true_positives[i].assign_add(tp)\n            self.false_positives[i].assign_add(fp)\n            self.false_negatives[i].assign_add(fn)\n            self.support[i].assign_add(support)\n\n    def result(self):\n        # Calculate F1 for each class and weight it\n        f1_scores = []\n        for i in range(self.num_classes):\n            tp = self.true_positives[i]\n            fp = self.false_positives[i]\n            fn = self.false_negatives[i]\n            support = self.support[i]\n\n            precision = tp / (tp + fp + tf.keras.backend.epsilon())\n            recall = tp / (tp + fn + tf.keras.backend.epsilon())\n            f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n            f1_scores.append(f1 * (support / tf.reduce_sum(self.support)))\n\n        return tf.reduce_sum(f1_scores)\n\n    def reset_states(self):\n        for v in self.variables:\n            v.assign(tf.zeros_like(v))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **MODEL DEFINITION**","metadata":{}},{"cell_type":"code","source":"@register_keras_serializable()\nclass CustomCastLayer(tfk.layers.Layer):\n    def call(self, inputs):\n        return tf.cast(inputs * 255, tf.uint8)\n\n@register_keras_serializable()\nclass CustomAugmentLayer(tfk.layers.Layer):\n    def __init__(self, max_rotation=30.0, **kwargs):\n        super(CustomAugmentLayer, self).__init__(**kwargs)\n        self.max_rotation = max_rotation / 360.00\n        \n    def call(self, inputs, training=False):\n        if training:\n            inputs = tf.image.random_flip_up_down(tf.image.random_flip_left_right(inputs))\n            inputs = tf.image.random_brightness(inputs, max_delta=0.1)\n            inputs = tf.image.random_contrast(inputs, lower=0.9, upper=1.1)\n            \n        return inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(shape=input_shape, n_labels=output_shape, convnext_trainable=False, #standard definitions\n                 n_dense_layers=1, initial_dense_neurons=1024, min_neurons=64, # architecture definitions\n                 include_dropout=False, dropout_rate=0.3, l2_lambda=0, # against overfitting\n                 learning_rate=6e-4):\n    \n    # Seed for reproducibility\n    tf.random.set_seed(seed)\n\n    # Metrics definition\n    weighted_macro_f1 = WeightedMacroF1Score(num_classes=n_labels)\n    METRICS = [tfk.metrics.SparseCategoricalAccuracy(), weighted_macro_f1]\n\n    # The input layer\n    inputs = tfkl.Input(shape=input_shape, name='Input')\n\n    # The two augmentation layers\n    x = CustomCastLayer()(inputs)\n    x = CustomAugmentLayer()(x, training=True)\n\n    # The convnext layer with include top=False to take the convolutional part only\n    convnext = tfk.applications.ConvNeXtXLarge(\n                input_shape=input_shape,\n                weights='imagenet',\n                include_top=False\n            )\n\n    # Here we freeze the convnext to perform Tranfer Learning\n    convnext.trainable = convnext_trainable\n\n    x = convnext(x)\n    x = tfkl.GlobalAveragePooling2D()(x)\n\n    # Hidden layers building\n    neurons = initial_dense_neurons\n    for k in range(n_dense_layers):\n        x = tfkl.Dense(units=neurons, activation='silu', name=f'Dense_layer_{k}', kernel_regularizer=tfk.regularizers.L2(l2_lambda))(x)\n        if include_dropout:\n            x = tfkl.Dropout(dropout_rate, name=f'Dropout_layer_{k}')(x)\n        neurons = max(neurons // 2, min_neurons)\n    outputs = tfkl.Dense(output_shape, activation='softmax', name='output_layer')(x)\n\n    # Final model building\n    model = tfk.Model(inputs=inputs, outputs=outputs, name='TF-CNN')\n\n    # Compile the model\n    loss = tfk.losses.SparseCategoricalCrossentropy()\n    optimizer = tfk.optimizers.AdamW(learning_rate, weight_decay=l2_lambda)\n    \n    model.compile(loss=loss, optimizer=optimizer, metrics=METRICS)\n\n    # Return the model\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **TRANSFER LEARNING**","metadata":{}},{"cell_type":"code","source":"# Best values found so far\nn_dense_layers = 2\ndropout_rate = 0.35\ninclude_dropout = True\nl2_lambda = 1e-2\n\nepochs = 100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the model with specified input and output shapes\nwith strategy.scope():\n    model = create_model(n_dense_layers=n_dense_layers, include_dropout=True, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n\n# Display a summary of the model architecture\nmodel.summary(expand_nested=False, show_trainable=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the patience value for early stopping\npatience = 35\n\n# Create an EarlyStopping callback\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor='val_loss',\n    mode='min',\n    patience=patience,\n    restore_best_weights=True\n)\n\nlr_reducer = CustomReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25, min_lr=1e-8)\n#plot_callback = RealTimePlot()\n\n# Store the callback in a list\ncallbacks = [lr_reducer, DisplayLearningRateCallback()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model with early stopping callback\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    shuffle=True,\n    callbacks=callbacks\n).history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation loss\nplt.figure(figsize=(15, 5))\nplt.plot(history['loss'], label='Training loss', alpha=.8)\nplt.plot(history['val_loss'], label='Validation loss', alpha=.8)\nplt.title('Loss')\nplt.legend()\nplt.grid(alpha=.3)\n\n# Plot training and validation accuracy\nplt.figure(figsize=(15, 5))\nplt.plot(history['binary_accuracy'], label='Training accuracy', alpha=.8)\nplt.plot(history['val_binary_accuracy'], label='Validation accuracy', alpha=.8)\nplt.title('Accuracy')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model to a file with the accuracy included in the filename\nwith strategy.scope():\n    model_weights_filename = 'MODEL.weights.h5'\n    model.save_weights(model_weights_filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LABELS = ['Basophil', 'Eosinophil', 'Erythroblast', 'Immature granulocytes', 'Lymphocyte', 'Monosyte', 'Neutrophil', 'Platelet']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **TRANSFER LEARNING EVALUATIONS**","metadata":{}},{"cell_type":"code","source":"def evaluations(model, ds, y_ds, labels, name):\n    # Predict class probabilities and get predicted classes\n    predictions = model.predict(ds, verbose=0)\n    predictions = np.argmax(predictions, axis=-1)\n    \n    # Extract ground truth classes\n    ds_gt = np.argmax(y_ds, axis=-1)\n    \n    # Calculate and display training set accuracy\n    ds_accuracy = accuracy_score(ds_gt, predictions)\n    print(f'Accuracy score over the {name} set: {round(ds_accuracy, 4)}')\n    \n    # Calculate and display training set precision\n    ds_precision = precision_score(train_gt, train_predictions, average='weighted')\n    print(f'Precision score over the {name} set: {round(ds_precision, 4)}')\n    \n    # Calculate and display training set recall\n    ds_recall = recall_score(ds_gt, ds_predictions, average='weighted')\n    print(f'Recall score over the {name} set: {round(ds_recall, 4)}')\n    \n    # Calculate and display training set F1 score\n    ds_f1 = f1_score(ds_gt, ds_predictions, average='weighted')\n    print(f'F1 score over the {name} set: {round(ds_f1, 4)}')\n    \n    # Compute the confusion matrix\n    cm = confusion_matrix(ds_gt, ds_predictions)\n    \n    # Create labels combining confusion matrix values\n    labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n    \n    # Plot the confusion matrix with class labels\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=labels, fmt='', xticklabels=labels, yticklabels=labels, cmap='Blues')\n    plt.xlabel('True labels')\n    plt.ylabel('Predicted labels')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_train, y_ds=y_train, name='training')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_val, y_ds=y_val, name='validation')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_test, y_ds=y_test, name='test')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **FINE TUNING**","metadata":{}},{"cell_type":"code","source":"# Best values found so far\nn_dense_layers = 2\ndropout_rate = 0.35\ninclude_dropout = True\nl2_lambda = 1e-2\n\nepochs = 30","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the model with specified input and output shapes\nwith strategy.scope():\n    model = create_model(n_dense_layers=n_dense_layers, include_dropout=True, dropout_rate=dropout_rate, l2_lambda=l2_lambda, convnext_trainable=True)\n\nmodel.trainable = True\nfor layer in model.get_layer('convnext_xlarge').layers:\n    if isinstance(layer, tfkl.LayerNormalization) or isinstance(layer, tfkl.Normalization):\n        layer.trainable = False\n        print(f\"Layer {layer.name}, trainable {layer.trainable}\")\n    if type(layer).__name__ == 'LayerScale':\n        layer.trainable = False\n        print(f\"Layer {layer.name}, trainable {layer.trainable}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_weights(\"MODEL.weights.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the patience value for early stopping\npatience = 12\n\n# Create an EarlyStopping callback\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor='val_binary_accuracy',\n    mode='max',\n    patience=patience,\n    restore_best_weights=True\n)\n\nlr_reducer = CustomReduceLROnPlateau(monitor='val_binary_accuracy', factor=0.5, patience=5, min_lr=1e-8)\nplot_callback = RealTimePlot()\n\n# Store the callback in a list\ncallbacks = [lr_reducer]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model with early stopping callback\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    callbacks=callbacks\n).history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **FINE TUNING EVALUATIONS**","metadata":{}},{"cell_type":"code","source":"evaluations(model, ds=X_train, y_ds=y_train, name='training')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_val, y_ds=y_val, name='validation')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluations(model, ds=X_test, y_ds=y_test, name='test')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}